#!/usr/bin/env python3
"""
Get-LLM-News ä¸»å…¥å£è„šæœ¬ã€‚

ä¸²è”å®Œæ•´çš„é‡‡é›† â†’ å»é‡ â†’ æ‘˜è¦ â†’ æ—¥æŠ¥ç”Ÿæˆæµç¨‹ã€‚

ç”¨æ³•:
    python main.py                          # è¿è¡Œæ‰€æœ‰æ•°æ®æº
    python main.py --sources hackernews     # åªè¿è¡Œ Hacker News
    python main.py --sources hackernews,reddit  # è¿è¡ŒæŒ‡å®šæ•°æ®æº
    python main.py --dry-run                # ä¸è°ƒç”¨ LLMï¼Œä»…é‡‡é›†æ•°æ®
    python main.py --days 3                 # å›æº¯ 3 å¤©
"""

from __future__ import annotations

import asyncio
import logging
import sys
from datetime import datetime, timezone
from pathlib import Path

import click

# ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨ sys.path ä¸­
PROJECT_ROOT = Path(__file__).resolve().parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from collectors.base import load_settings, load_kol_list, NewsItem
from collectors.hackernews import HackerNewsCollector
from collectors.reddit import RedditCollector
from collectors.twitter import TwitterCollector
from collectors.weibo_zhihu import WeiboZhihuCollector
from collectors.tech_news import TechNewsCollector
from processors.dedup import Deduplicator, sort_by_engagement
from processors.summarizer import Summarizer
from output.markdown_report import MarkdownReportGenerator


# æ•°æ®æºåç§° â†’ é‡‡é›†å™¨ç±»çš„æ˜ å°„
COLLECTOR_MAP = {
    "hackernews": HackerNewsCollector,
    "reddit": RedditCollector,
    "twitter": TwitterCollector,
    "weibo_zhihu": WeiboZhihuCollector,
    "tech_news": TechNewsCollector,
}

# æ‰€æœ‰æ•°æ®æº
ALL_SOURCES = list(COLLECTOR_MAP.keys())


def setup_logging(level: str = "INFO") -> None:
    """é…ç½®æ—¥å¿—ã€‚"""
    from rich.logging import RichHandler

    logging.basicConfig(
        level=getattr(logging, level.upper(), logging.INFO),
        format="%(message)s",
        datefmt="[%X]",
        handlers=[RichHandler(rich_tracebacks=True, show_path=False)],
    )


async def run_pipeline(
    sources: list[str],
    days: int = 1,
    dry_run: bool = False,
    max_items: int | None = None,
) -> Path | None:
    """
    æ‰§è¡Œå®Œæ•´çš„é‡‡é›†ç®¡é“ã€‚

    Args:
        sources: è¦é‡‡é›†çš„æ•°æ®æºåˆ—è¡¨
        days: å›æº¯å¤©æ•°
        dry_run: æ˜¯å¦è·³è¿‡ LLM æ‘˜è¦
        max_items: æ¯ä»½æŠ¥å‘Šæœ€å¤§æ¡ç›®æ•°

    Returns:
        ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å› None
    """
    logger = logging.getLogger("pipeline")

    # ===== 1. åŠ è½½é…ç½® =====
    logger.info("ğŸ“‹ åŠ è½½é…ç½®...")
    settings = load_settings()
    kol_config = load_kol_list()

    # è¦†ç›–å›æº¯å¤©æ•°
    if days:
        settings.setdefault("collection", {})["lookback_days"] = days

    if max_items:
        settings.setdefault("collection", {})["max_items_per_report"] = max_items

    max_report_items = settings.get("collection", {}).get("max_items_per_report", 50)

    # ===== 2. åˆå§‹åŒ–é‡‡é›†å™¨ =====
    logger.info(f"ğŸ”§ åˆå§‹åŒ–é‡‡é›†å™¨: {', '.join(sources)}")
    collectors = []
    for source_name in sources:
        if source_name not in COLLECTOR_MAP:
            logger.warning(f"æœªçŸ¥æ•°æ®æº: {source_name}ï¼Œè·³è¿‡")
            continue
        collector_cls = COLLECTOR_MAP[source_name]
        collectors.append(collector_cls(settings, kol_config))

    if not collectors:
        logger.error("æ²¡æœ‰æœ‰æ•ˆçš„é‡‡é›†å™¨ï¼Œé€€å‡º")
        return None

    # ===== 3. å¹¶è¡Œé‡‡é›† =====
    logger.info("ğŸš€ å¼€å§‹é‡‡é›†æ•°æ®...")
    tasks = [c.safe_collect() for c in collectors]
    results = await asyncio.gather(*tasks)

    # åˆå¹¶æ‰€æœ‰ç»“æœ
    all_items: list[NewsItem] = []
    for result in results:
        all_items.extend(result)

    logger.info(f"ğŸ“Š é‡‡é›†å®Œæˆï¼Œå…± {len(all_items)} æ¡åŸå§‹æ•°æ®")

    if not all_items:
        logger.warning("æœªé‡‡é›†åˆ°ä»»ä½•æ•°æ®ï¼Œç”Ÿæˆç©ºæŠ¥å‘Š")

    # ===== 4. å»é‡ =====
    logger.info("ğŸ” æ•°æ®å»é‡...")
    deduplicator = Deduplicator(similarity_threshold=0.75)
    unique_items = deduplicator.deduplicate(all_items)
    logger.info(f"å»é‡å: {len(unique_items)} æ¡ï¼ˆå»é™¤ {len(all_items) - len(unique_items)} æ¡é‡å¤ï¼‰")

    # æ’åºå¹¶æˆªå– top N
    unique_items = sort_by_engagement(unique_items)[:max_report_items]

    # ===== 5. LLM æ‘˜è¦ =====
    daily_summary = ""
    if not dry_run:
        logger.info("ğŸ¤– ç”Ÿæˆ LLM æ™ºèƒ½æ‘˜è¦...")
        summarizer = Summarizer(settings)

        # ç”Ÿæˆé€æ¡æ‘˜è¦
        await summarizer.summarize_items(unique_items)

        # ç”Ÿæˆæ—¥æŠ¥æ€»ç»“
        daily_summary = await summarizer.generate_daily_summary(unique_items)
        logger.info("âœ… LLM æ‘˜è¦ç”Ÿæˆå®Œæˆ")
    else:
        logger.info("â­ï¸ è·³è¿‡ LLM æ‘˜è¦ï¼ˆdry-run æ¨¡å¼ï¼‰")

    # ===== 6. ç”Ÿæˆ Markdown æ—¥æŠ¥ =====
    logger.info("ğŸ“ ç”Ÿæˆ Markdown æ—¥æŠ¥...")
    today = datetime.now(timezone.utc).strftime("%Y-%m-%d")
    report_gen = MarkdownReportGenerator(settings)
    report_path = report_gen.save(unique_items, daily_summary, today)

    logger.info(f"âœ… æ—¥æŠ¥å·²ä¿å­˜: {report_path}")

    # æ‰“å°æ‘˜è¦ç»Ÿè®¡
    _print_summary_stats(unique_items, logger)

    return report_path


def _print_summary_stats(items: list[NewsItem], logger: logging.Logger) -> None:
    """æ‰“å°é‡‡é›†ç»Ÿè®¡ä¿¡æ¯ã€‚"""
    if not items:
        return

    from collections import Counter

    source_counts = Counter(item.source for item in items)
    product_counts: Counter = Counter()
    for item in items:
        for tag in item.tags:
            product_counts[tag] += 1

    kol_count = sum(1 for item in items if item.is_kol)

    logger.info("\nğŸ“Š é‡‡é›†ç»Ÿè®¡:")
    logger.info(f"  æ€»æ¡ç›®: {len(items)}")
    logger.info(f"  KOL æ¡ç›®: {kol_count}")

    logger.info("  æŒ‰æ¥æº:")
    for source, count in source_counts.most_common():
        logger.info(f"    {source}: {count}")

    if product_counts:
        logger.info("  æŒ‰äº§å“:")
        for product, count in product_counts.most_common():
            logger.info(f"    {product}: {count}")


@click.command()
@click.option(
    "--sources",
    "-s",
    default=",".join(ALL_SOURCES),
    help=f"é€—å·åˆ†éš”çš„æ•°æ®æºåˆ—è¡¨ã€‚å¯é€‰: {', '.join(ALL_SOURCES)}",
)
@click.option(
    "--days",
    "-d",
    default=1,
    type=int,
    help="å›æº¯å¤©æ•°ï¼ˆé‡‡é›†æœ€è¿‘ N å¤©çš„æ•°æ®ï¼‰",
)
@click.option(
    "--dry-run",
    is_flag=True,
    default=False,
    help="ä¸è°ƒç”¨ LLM APIï¼Œä»…é‡‡é›†æ•°æ®å’Œç”ŸæˆåŸå§‹æŠ¥å‘Š",
)
@click.option(
    "--max-items",
    "-n",
    default=None,
    type=int,
    help="æ¯ä»½æŠ¥å‘Šæœ€å¤§æ¡ç›®æ•°",
)
@click.option(
    "--log-level",
    "-l",
    default="INFO",
    type=click.Choice(["DEBUG", "INFO", "WARNING", "ERROR"], case_sensitive=False),
    help="æ—¥å¿—çº§åˆ«",
)
def main(
    sources: str,
    days: int,
    dry_run: bool,
    max_items: int | None,
    log_level: str,
) -> None:
    """
    ğŸ¤– Get-LLM-News â€” AI ç¼–ç¨‹å·¥å…·ç¤¾äº¤åª’ä½“èˆ†æƒ…é‡‡é›†ç³»ç»Ÿ

    ä» Twitter/Xã€Redditã€Hacker Newsã€å¾®åš/çŸ¥ä¹ã€æŠ€æœ¯æ–°é—»ç«™é‡‡é›†
    Claudeã€Codexã€GitHub Copilot ç­‰ AI ç¼–ç¨‹å·¥å…·çš„æœ€æ–°åŠ¨æ€å’Œ KOL è§‚ç‚¹ã€‚
    """
    setup_logging(log_level)
    logger = logging.getLogger("main")

    # è§£ææ•°æ®æºåˆ—è¡¨
    source_list = [s.strip() for s in sources.split(",") if s.strip()]

    logger.info("=" * 60)
    logger.info("ğŸ¤– Get-LLM-News â€” AI ç¼–ç¨‹å·¥å…·èˆ†æƒ…é‡‡é›†")
    logger.info("=" * 60)
    logger.info(f"æ•°æ®æº: {', '.join(source_list)}")
    logger.info(f"å›æº¯å¤©æ•°: {days}")
    logger.info(f"Dry-run: {dry_run}")
    logger.info("")

    # è¿è¡Œå¼‚æ­¥ç®¡é“
    report_path = asyncio.run(
        run_pipeline(
            sources=source_list,
            days=days,
            dry_run=dry_run,
            max_items=max_items,
        )
    )

    if report_path:
        logger.info(f"\nğŸ‰ å®Œæˆï¼æŠ¥å‘Šè·¯å¾„: {report_path}")
    else:
        logger.error("\nâŒ ç®¡é“æ‰§è¡Œå¤±è´¥")
        sys.exit(1)


if __name__ == "__main__":
    main()
