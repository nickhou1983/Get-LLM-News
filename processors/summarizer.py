"""
LLM æ™ºèƒ½æ‘˜è¦ç”Ÿæˆå™¨ã€‚

ä½¿ç”¨ Claude (Anthropic) æˆ– GPT (OpenAI) å¯¹é‡‡é›†åˆ°çš„ä¿¡æ¯è¿›è¡Œï¼š
1. é€æ¡ä¸€å¥è¯æ‘˜è¦
2. æŒ‰äº§å“åˆ†ç±»çš„è¶‹åŠ¿æ€»ç»“
3. KOL æ ¸å¿ƒè§‚ç‚¹æç‚¼
4. å½“æ—¥çƒ­ç‚¹åˆ¤æ–­å’Œæ•´ä½“åˆ†æ

æ”¯æŒ Token é¢„ç®—æ§åˆ¶ï¼Œé¿å…æˆæœ¬å¤±æ§ã€‚
"""

from __future__ import annotations

import json
import logging
import os
from datetime import datetime

from collectors.base import NewsItem

logger = logging.getLogger(__name__)


# ===== Prompt æ¨¡æ¿ =====

DAILY_SUMMARY_PROMPT = """\
ä½ æ˜¯ä¸€ä¸ª AI ç¼–ç¨‹å·¥å…·è¡Œä¸šåˆ†æå¸ˆã€‚è¯·æ ¹æ®ä»¥ä¸‹ä»ç¤¾äº¤åª’ä½“å’ŒæŠ€æœ¯æ–°é—»é‡‡é›†åˆ°çš„ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä»½ç»“æ„åŒ–çš„ä¸­æ–‡æ—¥æŠ¥æ‘˜è¦ã€‚

## é‡‡é›†åˆ°çš„ä¿¡æ¯

{items_text}

## è¦æ±‚

è¯·ç”Ÿæˆä»¥ä¸‹å†…å®¹ï¼ˆä½¿ç”¨ä¸­æ–‡ï¼‰ï¼š

### 1. ä»Šæ—¥çƒ­ç‚¹ï¼ˆ2-3æ¡æœ€é‡è¦çš„æ–°é—»/åŠ¨æ€ï¼‰
- æ¯æ¡çƒ­ç‚¹ç”¨ 1-2 å¥è¯æ¦‚æ‹¬
- æ ‡æ³¨ç›¸å…³äº§å“å’Œæ¥æº

### 2. äº§å“åŠ¨æ€æ€»ç»“
é’ˆå¯¹æ¯ä¸ªè¢«æåŠçš„äº§å“ï¼ˆClaude, GitHub Copilot, Codex, Cursor, Windsurf ç­‰ï¼‰ï¼Œæ€»ç»“ï¼š
- æ–°åŠŸèƒ½/æ›´æ–°
- ç”¨æˆ·åé¦ˆå’Œè¯„ä»·
- å·²çŸ¥é—®é¢˜æˆ–äº‰è®®

### 3. KOL æ ¸å¿ƒè§‚ç‚¹
æç‚¼ KOLï¼ˆæ ‡è®°ä¸º [KOL] çš„æ¡ç›®ï¼‰çš„å…³é”®è§‚ç‚¹ï¼ŒåŒ…æ‹¬ï¼š
- è°è¯´äº†ä»€ä¹ˆï¼ˆç®€çŸ­å¼•è¿°ï¼‰
- è§‚ç‚¹çš„æ ¸å¿ƒç«‹åœº

### 4. è¶‹åŠ¿åˆ†æ
åŸºäºæ‰€æœ‰é‡‡é›†çš„ä¿¡æ¯ï¼Œåˆ†æï¼š
- è¡Œä¸šè¶‹åŠ¿ä¿¡å·
- å€¼å¾—å…³æ³¨çš„å˜åŒ–
- å¯¹å¼€å‘è€…çš„å»ºè®®

### 5. æƒ…æ„Ÿåˆ†ææ¦‚è§ˆ
å¯¹ä¸»è¦äº§å“çš„èˆ†æƒ…é£å‘åšç®€çŸ­åˆ¤æ–­ï¼ˆæ­£é¢/ä¸­æ€§/è´Ÿé¢ï¼‰

æ³¨æ„ï¼š
- ä¿æŒå®¢è§‚ä¸­ç«‹
- ç”¨ä¸­æ–‡è¾“å‡º
- ä¸è¦ç¼–é€ ä¿¡æ¯ï¼Œä»…åŸºäºæä¾›çš„æ•°æ®åˆ†æ
- å¦‚æœæŸä¸ªç»´åº¦æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ ‡æ³¨"æš‚æ— ç›¸å…³æ•°æ®"
"""

ITEM_SUMMARY_PROMPT = """\
è¯·ä¸ºä»¥ä¸‹å†…å®¹ç”Ÿæˆä¸€å¥è¯ä¸­æ–‡æ‘˜è¦ï¼ˆä¸è¶…è¿‡50å­—ï¼‰ï¼Œå¹¶åˆ¤æ–­æƒ…æ„Ÿå€¾å‘ï¼ˆpositive/neutral/negativeï¼‰ã€‚

æ ‡é¢˜: {title}
å†…å®¹: {content}
æ¥æº: {source}
ä½œè€…: {author}

è¯·ç”¨ä»¥ä¸‹ JSON æ ¼å¼å›å¤ï¼š
{{"summary": "ä¸€å¥è¯æ‘˜è¦", "sentiment": "positive/neutral/negative"}}
"""


class Summarizer:
    """LLM æ™ºèƒ½æ‘˜è¦ç”Ÿæˆå™¨ã€‚"""

    def __init__(self, settings: dict):
        self.settings = settings
        summarizer_cfg = settings.get("summarizer", {})
        self.provider = os.getenv("LLM_PROVIDER", summarizer_cfg.get("provider", "claude"))
        self.claude_model = summarizer_cfg.get("claude_model", "claude-sonnet-4-20250514")
        self.openai_model = summarizer_cfg.get("openai_model", "gpt-4o")
        self.max_tokens = summarizer_cfg.get("max_tokens", 4096)
        self.temperature = summarizer_cfg.get("temperature", 0.3)

    async def generate_daily_summary(self, items: list[NewsItem]) -> str:
        """
        ç”Ÿæˆæ—¥æŠ¥æ‘˜è¦æ–‡æœ¬ã€‚

        Args:
            items: å»é‡åçš„ NewsItem åˆ—è¡¨

        Returns:
            Markdown æ ¼å¼çš„æ‘˜è¦æ–‡æœ¬
        """
        if not items:
            return "ä»Šæ—¥æš‚æ— ç›¸å…³ä¿¡æ¯é‡‡é›†åˆ°ã€‚"

        # æ„å»ºè¾“å…¥æ–‡æœ¬
        items_text = self._format_items_for_prompt(items)
        prompt = DAILY_SUMMARY_PROMPT.format(items_text=items_text)

        # è°ƒç”¨ LLM
        summary = await self._call_llm(prompt)
        return summary

    async def summarize_items(self, items: list[NewsItem]) -> list[NewsItem]:
        """
        ä¸ºæ¯æ¡ NewsItem ç”Ÿæˆä¸€å¥è¯æ‘˜è¦å’Œæƒ…æ„Ÿåˆ†æã€‚

        ç›´æ¥ä¿®æ”¹ä¼ å…¥çš„ itemsï¼ˆè®¾ç½® summary å’Œ sentiment å­—æ®µï¼‰ã€‚
        æ‰¹é‡å¤„ç†ä»¥èŠ‚çœ API è°ƒç”¨ã€‚
        """
        if not items:
            return items

        # æ‰¹é‡å¤„ç†ï¼šå°†å¤šæ¡åˆå¹¶ä¸ºä¸€æ¬¡ API è°ƒç”¨
        batch_size = 10
        for i in range(0, len(items), batch_size):
            batch = items[i : i + batch_size]
            await self._summarize_batch(batch)

        return items

    async def _summarize_batch(self, items: list[NewsItem]) -> None:
        """æ‰¹é‡ä¸ºä¸€ç»„æ¡ç›®ç”Ÿæˆæ‘˜è¦ã€‚"""
        batch_prompt = "è¯·ä¸ºä»¥ä¸‹æ¯æ¡å†…å®¹ç”Ÿæˆä¸€å¥è¯ä¸­æ–‡æ‘˜è¦ï¼ˆä¸è¶…è¿‡50å­—ï¼‰å’Œæƒ…æ„Ÿå€¾å‘åˆ¤æ–­ã€‚\n\n"

        for idx, item in enumerate(items):
            batch_prompt += f"## æ¡ç›® {idx + 1}\n"
            batch_prompt += f"æ ‡é¢˜: {item.title[:200]}\n"
            batch_prompt += f"å†…å®¹: {item.content[:300]}\n"
            batch_prompt += f"æ¥æº: {item.source}\n\n"

        batch_prompt += (
            "\nè¯·ç”¨ JSON æ•°ç»„æ ¼å¼å›å¤ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« index, summary, sentiment å­—æ®µã€‚\n"
            'ä¾‹å¦‚: [{"index": 1, "summary": "æ‘˜è¦", "sentiment": "positive"}]\n'
            "åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–æ–‡æœ¬ã€‚"
        )

        try:
            response = await self._call_llm(batch_prompt)
            # å°è¯•è§£æ JSON
            # ç§»é™¤å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°
            clean_response = response.strip()
            if clean_response.startswith("```"):
                clean_response = clean_response.split("\n", 1)[-1]
                if clean_response.endswith("```"):
                    clean_response = clean_response[:-3]

            results = json.loads(clean_response)

            for result in results:
                idx = result.get("index", 0) - 1
                if 0 <= idx < len(items):
                    items[idx].summary = result.get("summary", "")
                    items[idx].sentiment = result.get("sentiment", "neutral")

        except (json.JSONDecodeError, KeyError, TypeError) as e:
            logger.warning(f"æ‰¹é‡æ‘˜è¦è§£æå¤±è´¥: {e}")
            # é™çº§ï¼šè®¾ç½®é»˜è®¤å€¼
            for item in items:
                if not item.summary:
                    item.summary = item.title[:50]
                if not item.sentiment:
                    item.sentiment = "neutral"

    async def _call_llm(self, prompt: str) -> str:
        """è°ƒç”¨ LLM APIã€‚"""
        if self.provider == "claude":
            return await self._call_claude(prompt)
        elif self.provider == "openai":
            return await self._call_openai(prompt)
        else:
            logger.warning(f"æœªçŸ¥ LLM æä¾›å•†: {self.provider}ï¼Œå°è¯• Claude")
            return await self._call_claude(prompt)

    async def _call_claude(self, prompt: str) -> str:
        """è°ƒç”¨ Anthropic Claude APIã€‚"""
        api_key = os.getenv("ANTHROPIC_API_KEY", "")
        if not api_key:
            logger.warning("æœªé…ç½® ANTHROPIC_API_KEYï¼Œè·³è¿‡ LLM æ‘˜è¦")
            return self._generate_fallback_summary(prompt)

        try:
            import anthropic

            client = anthropic.AsyncAnthropic(api_key=api_key)

            message = await client.messages.create(
                model=self.claude_model,
                max_tokens=self.max_tokens,
                temperature=self.temperature,
                messages=[
                    {"role": "user", "content": prompt}
                ],
            )

            return message.content[0].text

        except ImportError:
            logger.warning("anthropic åº“æœªå®‰è£…ï¼Œå°è¯• OpenAI ä½œä¸ºå¤‡ä»½")
            return await self._call_openai(prompt)
        except Exception as e:
            logger.error(f"Claude API è°ƒç”¨å¤±è´¥: {e}")
            # Fallback to OpenAI
            if os.getenv("OPENAI_API_KEY"):
                logger.info("åˆ‡æ¢åˆ° OpenAI ä½œä¸ºå¤‡ä»½")
                return await self._call_openai(prompt)
            return self._generate_fallback_summary(prompt)

    async def _call_openai(self, prompt: str) -> str:
        """è°ƒç”¨ OpenAI APIã€‚"""
        api_key = os.getenv("OPENAI_API_KEY", "")
        if not api_key:
            logger.warning("æœªé…ç½® OPENAI_API_KEYï¼Œè·³è¿‡ LLM æ‘˜è¦")
            return self._generate_fallback_summary(prompt)

        try:
            from openai import AsyncOpenAI

            client = AsyncOpenAI(api_key=api_key)

            response = await client.chat.completions.create(
                model=self.openai_model,
                max_tokens=self.max_tokens,
                temperature=self.temperature,
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ª AI ç¼–ç¨‹å·¥å…·è¡Œä¸šåˆ†æå¸ˆï¼Œæ“…é•¿ä»ç¤¾äº¤åª’ä½“å’Œæ–°é—»ä¸­æç‚¼å…³é”®ä¿¡æ¯ã€‚",
                    },
                    {"role": "user", "content": prompt},
                ],
            )

            return response.choices[0].message.content or ""

        except ImportError:
            logger.error("openai åº“æœªå®‰è£…")
            return self._generate_fallback_summary(prompt)
        except Exception as e:
            logger.error(f"OpenAI API è°ƒç”¨å¤±è´¥: {e}")
            return self._generate_fallback_summary(prompt)

    def _format_items_for_prompt(self, items: list[NewsItem]) -> str:
        """å°† NewsItem åˆ—è¡¨æ ¼å¼åŒ–ä¸º prompt è¾“å…¥æ–‡æœ¬ã€‚"""
        lines = []
        for idx, item in enumerate(items[:50], 1):  # é™åˆ¶æœ€å¤š 50 æ¡
            kol_tag = " [KOL]" if item.is_kol else ""
            products = ", ".join(item.tags) if item.tags else "æœªåˆ†ç±»"
            lines.append(
                f"### {idx}. [{item.source}]{kol_tag} {item.title}\n"
                f"- ä½œè€…: {item.author} ({item.author_handle})\n"
                f"- äº§å“: {products}\n"
                f"- äº’åŠ¨: ğŸ‘{item.engagement} ğŸ’¬{item.comments_count}\n"
                f"- é“¾æ¥: {item.url}\n"
                f"- å†…å®¹æ‘˜è¦: {item.content[:300]}\n"
            )
        return "\n".join(lines)

    @staticmethod
    def _generate_fallback_summary(prompt: str) -> str:
        """å½“ LLM API ä¸å¯ç”¨æ—¶çš„é™çº§æ‘˜è¦ï¼ˆç®€å•çš„ç»Ÿè®¡ä¿¡æ¯ï¼‰ã€‚"""
        return (
            "> âš ï¸ LLM API æœªé…ç½®æˆ–è°ƒç”¨å¤±è´¥ï¼Œä»¥ä¸‹ä¸ºåŸå§‹æ•°æ®æ±‡æ€»ã€‚\n"
            "> è¯·é…ç½® ANTHROPIC_API_KEY æˆ– OPENAI_API_KEY ä»¥è·å¾—æ™ºèƒ½æ‘˜è¦ã€‚\n\n"
            "è¯·æŸ¥çœ‹ä¸‹æ–¹å„æ•°æ®æºçš„è¯¦ç»†æ¡ç›®ã€‚"
        )
